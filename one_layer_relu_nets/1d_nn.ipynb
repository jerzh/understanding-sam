{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f05057a5d10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "torch.manual_seed(5)    # fix random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.Tensor((np.array([-5.5,-4.5,-3.5,-2.5,-1.5,-0.5,0.5,1.5,2.5,3.5,4.5,5.5])/4+0.07).reshape(-1,1))  # same data as paper by Maennel\n",
    "y = torch.Tensor(.345 + np.array([-2, 1.5, 0, 0, 2, 1, 0, -1, 0, 1, 2, -1]).reshape(-1,1)) \n",
    "\n",
    "# torch can only train on Variable, so convert them to Variable\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "# view data\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    1 hidden layer Relu network\n",
    "    \"\"\"\n",
    "    def __init__(self, n_feature, n_hidden, n_output, init_scale=1, bias=True):\n",
    "        \"\"\"\n",
    "        n_feature: dimension of input\n",
    "        n_hidden: number of hidden neurons\n",
    "        n_output: dimension of output\n",
    "        init_scale: all the weights are initialized ~ N(0, init_scale^2/m) where m is the input dimension of this layer\n",
    "        bias: if True, use bias parameters in all layers. Use no bias otherwise\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.init_scale = init_scale\n",
    "        \n",
    "        # The initialization follows the one from Blanc et al.\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden, bias=bias)   # hidden layer with rescaled init\n",
    "        self.hidden.weight.data = torch.rand_like(self.hidden.weight)\n",
    "        self.hidden.weight.data = 2.5 * (-1 + 2 * torch.round(self.hidden.weight.data)) * init_scale\n",
    "        self.hidden.bias.data = torch.randn_like(self.hidden.bias)\n",
    "        self.hidden.bias.data = self.hidden.bias.data * 2.5 * init_scale\n",
    "        \n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output, bias=False)   # output layer with rescaled init\n",
    "        self.predict.weight.data = torch.randn_like(self.predict.weight)\n",
    "        self.predict.weight.data = self.predict.weight.data * 4.0 * init_scale\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.hidden(z))      # activation function for hidden layer\n",
    "        z = self.predict(z)             # linear output\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in [0, 1, 2, 3, 4]:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    init_scale = 1.0\n",
    "    net = Net(n_feature=1, n_hidden=100, n_output=1, init_scale=init_scale)     # define the network\n",
    "\n",
    "    lr_decay = True\n",
    "    sam_grad_normalization = True\n",
    "    lr_max = 0.00005  \n",
    "    rho = 0.0  # use 0.3 for SAM\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr_max)#, momentum=0.9)\n",
    "    loss_func = torch.nn.MSELoss(reduction='sum')  # this is for regression mean squared loss\n",
    "\n",
    "    n_samples = x.shape[0]\n",
    "    iter_max = 1000000\n",
    "    n_batch = n_samples # batch size for SGD\n",
    "\n",
    "    loss = torch.Tensor(np.array(np.inf))\n",
    "    # train the network\n",
    "    n_iterations = 0 # number of descent steps\n",
    "    while n_iterations<iter_max+1 and loss > 10**-5:\n",
    "        n_iterations += 1\n",
    "        # optimizer.param_groups[0]['lr'] = lr_max * (0.5 + 0.5*math.cos(math.pi * n_iterations / iter_max))\n",
    "        indices = np.random.choice(n_samples, size=n_batch, replace=False)\n",
    "        batch_x, batch_y = x[indices], y[indices]\n",
    "\n",
    "        prediction = net(batch_x)     # input x and predict based on x\n",
    "        loss = 0.5*loss_func(prediction, batch_y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "        if n_iterations % 2000 == 1:\n",
    "            print(n_iterations, loss)\n",
    "\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "\n",
    "        if rho > 0.0:\n",
    "            orig_weights = [param.clone() for param in list(net.parameters())]\n",
    "            optimizer.param_groups[0]['lr'] = -rho\n",
    "\n",
    "            if sam_grad_normalization:\n",
    "                with torch.no_grad():\n",
    "                    grad_norm = 0\n",
    "                    for param_curr in net.parameters():\n",
    "                        grad_norm += (param_curr.grad**2).sum() \n",
    "                    grad_norm = grad_norm**0.5\n",
    "                    for param_curr in net.parameters():\n",
    "                        param_curr.grad /= grad_norm\n",
    "            # print(net.hidden.weight.grad[:5])\n",
    "            optimizer.step()        # descent step\n",
    "            \n",
    "            if lr_decay:\n",
    "                # optimizer.param_groups[0]['lr'] = lr_max * (0.5 + 0.5*math.cos(math.pi * n_iterations / iter_max))\n",
    "                optimizer.param_groups[0]['lr'] = lr_max * (1 - n_iterations / iter_max)\n",
    "            else:\n",
    "                optimizer.param_groups[0]['lr'] = lr_max\n",
    "\n",
    "            prediction = net(batch_x)     # input x and predict based on x\n",
    "            loss = 0.5*loss_func(prediction, batch_y)     # must be (1. nn output, 2. target)\n",
    "        \n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "\n",
    "            for param_curr, param_orig, in zip(list(net.parameters()), orig_weights):\n",
    "                param_curr.data = param_orig.data\n",
    "        # print(net.hidden.weight.grad[:5])\n",
    "        optimizer.step()        # descent step\n",
    "\n",
    "    print(n_iterations, loss)\n",
    "\n",
    "    torch.save(net, 'models_simple/model-lr={}-rho={}-seed={}.pt'.format(lr_max, rho, seed))\n",
    "\n",
    "    # plot and show learning process\n",
    "    plt.figure()\n",
    "    # plt.title('Regression')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.scatter(x.data.numpy(), y.data.numpy(), color=\"orange\")\n",
    "    plt.ylim(1.3*y.data.numpy().min(), 1.2*y.data.numpy().max())\n",
    "    grid_step = 0.01\n",
    "    x_grid = np.arange(x.data.numpy().min() - 5*grid_step, x.data.numpy().max() + 5*grid_step, grid_step)\n",
    "    y_preds = net(torch.Tensor(x_grid.reshape(-1, 1))).detach()\n",
    "    plt.plot(x_grid, y_preds, 'g-', lw=3)\n",
    "    plt.show()\n",
    "    print(n_iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
